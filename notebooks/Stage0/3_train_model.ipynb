{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46ff05d",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff154fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b8d5c",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_ROOT = Path(\"../../data/processed/Stage0\")\n",
    "INDEX_CSV      = PROCESSED_ROOT / \"index.csv\"\n",
    "\n",
    "TRAIN_LABELS_JSON = Path(\"../../data/labels/Stage0/train.json\")\n",
    "VAL_LABELS_JSON   = Path(\"../../data/labels/Stage0/val.json\")\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE  = 55\n",
    "LR          = 1e-3\n",
    "EPOCHS      = 7\n",
    "IMAGE_SIZE  = 256\n",
    "\n",
    "# Choose \"rgb\" (3-channel) or \"gray\" (1-channel)\n",
    "IMAGE_MODE = \"rgb\"   # change to \"gray\" if you want grayscale training\n",
    "\n",
    "MODEL_OUT_PATH = Path(\"../../models/classifier/Stage0/model.pt\")\n",
    "MODEL_OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee661348",
   "metadata": {},
   "source": [
    "Load index.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6bf3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in index.csv: 375\n",
      "Example row keys: ['filepath', 'split', 'stage', 'scan_session_id', 'group_id', 'filename', 'scan_timestamp', 'source_interim_path']\n",
      "Example filepath: images/train/2026-01-21_10-32-40-301_aug002.png\n"
     ]
    }
   ],
   "source": [
    "def read_index_csv(index_csv_path: Path):\n",
    "    rows = []\n",
    "    with open(index_csv_path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    if not rows:\n",
    "        raise ValueError(f\"index.csv is empty: {index_csv_path}\")\n",
    "    if \"filepath\" not in rows[0]:\n",
    "        raise ValueError(\"index.csv must contain a 'filepath' column\")\n",
    "    return rows\n",
    "\n",
    "index_rows = read_index_csv(INDEX_CSV)\n",
    "print(\"Rows in index.csv:\", len(index_rows))\n",
    "print(\"Example row keys:\", list(index_rows[0].keys()))\n",
    "print(\"Example filepath:\", index_rows[0][\"filepath\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57827cd",
   "metadata": {},
   "source": [
    "Load label JSONs (train/val)\n",
    "\n",
    "This expects a simple dict mapping:\n",
    "\n",
    "{\n",
    "  \"images/train/xxx.png\": 0,\n",
    "  \"images/train/yyy.png\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd4959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train label: 2026-01-21_10-32-05-130_aug00.png -> 1\n",
      "Example train label: 2026-01-21_10-32-05-130_aug000.png -> 1\n",
      "Example train label: 2026-01-21_10-32-05-130_aug001.png -> 1\n"
     ]
    }
   ],
   "source": [
    "def load_label_map(label_path: Path):\n",
    "    if not label_path.exists():\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "    with open(label_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict):\n",
    "        # Original format: {filepath: class_id}\n",
    "        out = {}\n",
    "        for k, v in data.items():\n",
    "            out[k.replace(\"\\\\\", \"/\")] = int(v)\n",
    "        return out\n",
    "    elif isinstance(data, list):\n",
    "        out = {}\n",
    "        for item in data:\n",
    "            filepath = item[\"image\"]\n",
    "            # tray_mask = 1 means object present\n",
    "            class_id = 1 if item.get(\"empty\", 0) == 1 else 0\n",
    "            out[filepath] = class_id\n",
    "        return out\n",
    "    else:\n",
    "        raise ValueError(\"Label JSON must be a dict {filepath: class_id} or Label Studio list format\")\n",
    "\n",
    "\n",
    "train_label_map = load_label_map(TRAIN_LABELS_JSON)\n",
    "val_label_map   = load_label_map(VAL_LABELS_JSON)\n",
    "\n",
    "# Show a couple samples\n",
    "for i, (k, v) in enumerate(train_label_map.items()):\n",
    "    print(\"Example train label:\", k, \"->\", v)\n",
    "    if i >= 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a89e6e",
   "metadata": {},
   "source": [
    "Define Dataset (respects split, no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a562653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedSplitDataset(Dataset):\n",
    "    def __init__(self, index_rows, processed_root: Path, split: str, label_map: dict, transform=None):\n",
    "        self.processed_root = processed_root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "        # Filter filepaths by split\n",
    "        filepaths = []\n",
    "        for r in index_rows:\n",
    "            fp = r[\"filepath\"].replace(\"\\\\\", \"/\")\n",
    "            row_split = (r.get(\"split\") or \"\").strip().lower()\n",
    "\n",
    "            if row_split:\n",
    "                if row_split == split:\n",
    "                    filepaths.append(fp)\n",
    "            else:\n",
    "                # fallback if index.csv doesn't have split column\n",
    "                if f\"images/{split}/\" in fp:\n",
    "                    filepaths.append(fp)\n",
    "\n",
    "        if not filepaths:\n",
    "            raise ValueError(f\"No samples found for split='{split}'\")\n",
    "\n",
    "        # Keep only those that have labels (strict)\n",
    "        self.filepaths = []\n",
    "        self.labels = []\n",
    "        for fp in filepaths:\n",
    "            filename = Path(fp).name\n",
    "            label = self.label_map.get(filename, 0)  # default to 0 if missing\n",
    "            self.filepaths.append(fp)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.filepaths[idx]\n",
    "        img_path = self.processed_root / rel_path\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Force consistent mode\n",
    "        if IMAGE_MODE == \"gray\":\n",
    "            img = img.convert(\"L\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56b3e9",
   "metadata": {},
   "source": [
    "Transforms + DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a58308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 323\n",
      "Val samples: 35\n",
      "Batch image shape: torch.Size([15, 3, 256, 256]) | Batch label shape: torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "if IMAGE_MODE == \"gray\":\n",
    "    transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),  # -> [1, H, W]\n",
    "    ])\n",
    "    in_channels = 1\n",
    "else:\n",
    "    transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),  # -> [3, H, W]\n",
    "    ])\n",
    "    in_channels = 3\n",
    "\n",
    "train_ds = ProcessedSplitDataset(index_rows, PROCESSED_ROOT, \"train\", train_label_map, transform=transform)\n",
    "val_ds   = ProcessedSplitDataset(index_rows, PROCESSED_ROOT, \"val\",   val_label_map,   transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples:\", len(val_ds))\n",
    "\n",
    "# Quick check tensor shapes\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch image shape:\", x.shape, \"| Batch label shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a43623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ec6184",
   "metadata": {},
   "source": [
    "Define a simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2dbb0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN_GAP(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # /2\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),          # /4\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),          # /8\n",
    "        )\n",
    "        # IMAGE_SIZE is global; compute flattened size dynamically\n",
    "        # create a \"fake\" image -> pass to features -> measure output size automatically\n",
    "        # TO AVOID HARDCODING SIZES AND BREAKING MODEL WHEN IMAGE_SIZE CHANGES\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, IMAGE_SIZE, IMAGE_SIZE)\n",
    "            out = self.features(dummy)\n",
    "            flat_dim = out.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.2), # turn off 20% of neurons, prevent memorisation, prevent double-descent peak\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "\n",
    "# LESSER PARAMETER (BETTER FOR SMALL DATASET)\n",
    "class SimpleCNN_GAP(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "\n",
    "        # Global Average Pool: (B, 64, H, W) -> (B, 64, 1, 1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1) # (B, 64)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "\n",
    "#model = SimpleCNN(in_channels=in_channels, num_classes=NUM_CLASSES).to(device)\n",
    "model = SimpleCNN_GAP(in_channels=in_channels,num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Used for classification, compare predicted scores vs true label\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR) # Smart gradient descent, adjust learning rates automatically\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd13e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27874"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42f854",
   "metadata": {},
   "source": [
    "Training Loop (train + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec5860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Train Loss: 0.2516 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m train_loss_sum, train_correct, train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/xray-gen-ai_Project/xray-gen-ai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/xray-gen-ai_Project/xray-gen-ai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1482\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1482\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/xray-gen-ai_Project/xray-gen-ai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1444\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1444\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1446\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/xray-gen-ai_Project/xray-gen-ai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1275\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/snap/code/218/.local/share/uv/python/cpython-3.10.19-linux-x86_64-gnu/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/snap/code/218/.local/share/uv/python/cpython-3.10.19-linux-x86_64-gnu/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/snap/code/218/.local/share/uv/python/cpython-3.10.19-linux-x86_64-gnu/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/snap/code/218/.local/share/uv/python/cpython-3.10.19-linux-x86_64-gnu/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/snap/code/218/.local/share/uv/python/cpython-3.10.19-linux-x86_64-gnu/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss_sum, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss = train_loss_sum / max(1, train_total)\n",
    "    train_acc  = train_correct / max(1, train_total)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval()\n",
    "    val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss_sum += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / max(1, val_total)\n",
    "    val_acc  = val_correct / max(1, val_total)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch}/{EPOCHS}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f} \"\n",
    "        f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a09e6a",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: ../../models/classifier/Stage0/model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), MODEL_OUT_PATH)\n",
    "print(\"✅ Saved model to:\", MODEL_OUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray-gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
