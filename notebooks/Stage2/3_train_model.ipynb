{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46ff05d",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff154fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b8d5c",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1915336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_ROOT = Path(\"../../data/processed/Stage2\")\n",
    "INDEX_CSV      = PROCESSED_ROOT / \"index.csv\"\n",
    "\n",
    "TRAIN_LABELS_JSON = Path(\"../../data/labels/Stage2/train.json\")\n",
    "VAL_LABELS_JSON   = Path(\"../../data/labels/Stage2/val.json\")\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE  = 60\n",
    "LR          = 1e-3\n",
    "EPOCHS      = 16\n",
    "IMAGE_SIZE  = 256\n",
    "\n",
    "# Choose \"rgb\" (3-channel) or \"gray\" (1-channel)\n",
    "IMAGE_MODE = \"rgb\"   # change to \"gray\" if you want grayscale training\n",
    "\n",
    "MODEL_OUT_PATH = Path(\"../../models/classifier/Stage2/model.pt\")\n",
    "MODEL_OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee661348",
   "metadata": {},
   "source": [
    "Load index.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6bf3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in index.csv: 1100\n",
      "Example row keys: ['filepath', 'split', 'stage', 'scan_session_id', 'filename', 'scan_timestamp', 'source_interim_path']\n",
      "Example filepath: images/train/2026-01-21_15-51-50-237_aug01.png\n"
     ]
    }
   ],
   "source": [
    "def read_index_csv(index_csv_path: Path):\n",
    "    rows = []\n",
    "    with open(index_csv_path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    if not rows:\n",
    "        raise ValueError(f\"index.csv is empty: {index_csv_path}\")\n",
    "    if \"filepath\" not in rows[0]:\n",
    "        raise ValueError(\"index.csv must contain a 'filepath' column\")\n",
    "    return rows\n",
    "\n",
    "index_rows = read_index_csv(INDEX_CSV)\n",
    "print(\"Rows in index.csv:\", len(index_rows))\n",
    "print(\"Example row keys:\", list(index_rows[0].keys()))\n",
    "print(\"Example filepath:\", index_rows[0][\"filepath\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57827cd",
   "metadata": {},
   "source": [
    "Load label JSONs (train/val)\n",
    "\n",
    "This expects a simple dict mapping:\n",
    "\n",
    "{\n",
    "  \"images/train/xxx.png\": 0,\n",
    "  \"images/train/yyy.png\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd4959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: 935\n",
      "Val labels: 110\n",
      "Train label distribution: Counter({0: 935})\n",
      "Val label distribution: Counter({0: 110})\n",
      "Example train label: 2026-01-21_15-43-51-512_aug00.png -> 0\n",
      "Example train label: 2026-01-21_15-43-51-512_aug01.png -> 0\n",
      "Example train label: 2026-01-21_15-43-51-512_aug02.png -> 0\n"
     ]
    }
   ],
   "source": [
    "def load_label_map(label_path: Path):\n",
    "    if not label_path.exists():\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "    with open(label_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict):\n",
    "        # Original format: {filepath: class_id}\n",
    "        out = {}\n",
    "        for k, v in data.items():\n",
    "            out[k.replace(\"\\\\\", \"/\")] = int(v)\n",
    "        return out\n",
    "    elif isinstance(data, list):\n",
    "        # Custom format: list of {\"image\": filename, \"no_contraband\": 0/1, \"isolated_items\": 0/1}\n",
    "        out = {}\n",
    "        for item in data:\n",
    "            filepath = item['image']\n",
    "            # Assume class 0 for no_contraband=1, class 1 otherwise\n",
    "            class_id = 0 if item.get('no_contraband', 0) == 1 else 1\n",
    "            out[filepath] = class_id\n",
    "        return out\n",
    "    else:\n",
    "        raise ValueError(\"Label JSON must be a dict {filepath: class_id} or Label Studio list format\")\n",
    "\n",
    "train_label_map = load_label_map(TRAIN_LABELS_JSON)\n",
    "val_label_map   = load_label_map(VAL_LABELS_JSON)\n",
    "\n",
    "print(\"Train labels:\", len(train_label_map))\n",
    "print(\"Val labels:\", len(val_label_map))\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Train label distribution:\", Counter(train_label_map.values()))\n",
    "print(\"Val label distribution:\", Counter(val_label_map.values()))\n",
    "\n",
    "# Show a couple samples\n",
    "for i, (k, v) in enumerate(train_label_map.items()):\n",
    "    print(\"Example train label:\", k, \"->\", v)\n",
    "    if i >= 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a89e6e",
   "metadata": {},
   "source": [
    "Define Dataset (respects split, no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a562653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedSplitDataset(Dataset):\n",
    "    def __init__(self, index_rows, processed_root: Path, split: str, label_map: dict, transform=None):\n",
    "        self.processed_root = processed_root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "        # Filter filepaths by split\n",
    "        filepaths = []\n",
    "        for r in index_rows:\n",
    "            fp = r[\"filepath\"].replace(\"\\\\\", \"/\")\n",
    "            row_split = (r.get(\"split\") or \"\").strip().lower()\n",
    "\n",
    "            if row_split:\n",
    "                if row_split == split:\n",
    "                    filepaths.append(fp)\n",
    "            else:\n",
    "                # fallback if index.csv doesn't have split column\n",
    "                if f\"images/{split}/\" in fp:\n",
    "                    filepaths.append(fp)\n",
    "\n",
    "        if not filepaths:\n",
    "            raise ValueError(f\"No samples found for split='{split}'\")\n",
    "\n",
    "        # Keep only those that have labels (strict)\n",
    "        self.filepaths = []\n",
    "        self.labels = []\n",
    "        for fp in filepaths:\n",
    "            filename = Path(fp).name\n",
    "            label = self.label_map.get(filename, 0)  # default to 0 if missing\n",
    "            self.filepaths.append(fp)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.filepaths[idx]\n",
    "        img_path = self.processed_root / rel_path\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Force consistent mode\n",
    "        if IMAGE_MODE == \"gray\":\n",
    "            img = img.convert(\"L\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56b3e9",
   "metadata": {},
   "source": [
    "Transforms + DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a58308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 935\n",
      "Val samples: 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image shape: torch.Size([60, 3, 256, 256]) | Batch label shape: torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "if IMAGE_MODE == \"gray\":\n",
    "    transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),  # -> [1, H, W]\n",
    "    ])\n",
    "    in_channels = 1\n",
    "else:\n",
    "    transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),  # -> [3, H, W]\n",
    "    ])\n",
    "    in_channels = 3\n",
    "\n",
    "train_ds = ProcessedSplitDataset(index_rows, PROCESSED_ROOT, \"train\", train_label_map, transform=transform)\n",
    "val_ds   = ProcessedSplitDataset(index_rows, PROCESSED_ROOT, \"val\",   val_label_map,   transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples:\", len(val_ds))\n",
    "\n",
    "# Quick check tensor shapes\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch image shape:\", x.shape, \"| Batch label shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec6184",
   "metadata": {},
   "source": [
    "Train with the previous model from previous stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2dbb0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=65536, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # /2\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),          # /4\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),          # /8\n",
    "        )\n",
    "        # IMAGE_SIZE is global; compute flattened size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, IMAGE_SIZE, IMAGE_SIZE)\n",
    "            out = self.features(dummy)\n",
    "            flat_dim = out.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SimpleCNN(in_channels=in_channels, num_classes=NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(\"../../models/classifier/Stage1/model.pt\")) #IMPORTANT, LOAD THE MODEL FROM PREVIOUS STAGE\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42f854",
   "metadata": {},
   "source": [
    "Training Loop (train + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec5860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [2/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [3/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [4/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [5/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [6/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [7/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [8/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [9/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [10/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [11/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [12/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [13/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [14/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [15/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n",
      "Epoch [16/16] Train Loss: 0.0000 | Train Acc: 1.000 | Val Loss: 0.0000 | Val Acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss_sum, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss = train_loss_sum / max(1, train_total)\n",
    "    train_acc  = train_correct / max(1, train_total)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval()\n",
    "    val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss_sum += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / max(1, val_total)\n",
    "    val_acc  = val_correct / max(1, val_total)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch}/{EPOCHS}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f} \"\n",
    "        f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a09e6a",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec1e7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: ../../models/classifier/Stage2/model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), MODEL_OUT_PATH)\n",
    "print(\"✅ Saved model to:\", MODEL_OUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray-gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
