{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46ff05d",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff154fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b8d5c",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1915336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_ROOT = Path(\"../../data/processed/Stage1\")\n",
    "INDEX_CSV      = PROCESSED_ROOT / \"index.csv\"\n",
    "\n",
    "TRAIN_LABELS_JSON = Path(\"../../data/labels/Stage1/combined/train.json\")\n",
    "VAL_LABELS_JSON   = Path(\"../../data/labels/Stage1/combined/val.json\")\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE  = 70\n",
    "LR          = 1e-3\n",
    "EPOCHS      = 15\n",
    "IMAGE_SIZE  = 256\n",
    "\n",
    "# Choose \"rgb\" (3-channel) or \"gray\" (1-channel)\n",
    "IMAGE_MODE = \"rgb\"   # change to \"gray\" if you want grayscale training\n",
    "\n",
    "MODEL_OUT_PATH = Path(\"../../models/classifier/Stage1/model.pt\")\n",
    "MODEL_OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee661348",
   "metadata": {},
   "source": [
    "Load index.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6bf3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in index.csv: 1250\n",
      "Example row keys: ['filepath', 'split', 'stage', 'scan_session_id', 'group_id', 'filename', 'scan_timestamp', 'source_interim_path']\n",
      "Example filepath: images/train/2026-01-21_10-37-50-052_aug00.png\n"
     ]
    }
   ],
   "source": [
    "def read_index_csv(index_csv_path: Path):\n",
    "    rows = []\n",
    "    with open(index_csv_path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    if not rows:\n",
    "        raise ValueError(f\"index.csv is empty: {index_csv_path}\")\n",
    "    if \"filepath\" not in rows[0]:\n",
    "        raise ValueError(\"index.csv must contain a 'filepath' column\")\n",
    "    return rows\n",
    "\n",
    "index_rows = read_index_csv(INDEX_CSV)\n",
    "print(\"Rows in index.csv:\", len(index_rows))\n",
    "print(\"Example row keys:\", list(index_rows[0].keys()))\n",
    "print(\"Example filepath:\", index_rows[0][\"filepath\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57827cd",
   "metadata": {},
   "source": [
    "Load label JSONs (train/val)\n",
    "\n",
    "This expects a simple dict mapping:\n",
    "\n",
    "{\n",
    "  \"images/train/xxx.png\": 0,\n",
    "  \"images/train/yyy.png\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd4959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: 1063\n",
      "Val labels: 120\n",
      "Train label distribution: Counter({1: 740, 0: 323})\n",
      "Val label distribution: Counter({1: 85, 0: 35})\n",
      "Example train label: 2026-01-21_10-32-05-130_aug00.png -> 0\n",
      "Example train label: 2026-01-21_10-32-05-130_aug000.png -> 0\n",
      "Example train label: 2026-01-21_10-32-05-130_aug001.png -> 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def load_label_map(label_path: Path):\n",
    "    if not label_path.exists():\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # -------------------------\n",
    "    # Dict format: {filepath: class_id}\n",
    "    # -------------------------\n",
    "    if isinstance(data, dict):\n",
    "        out = {}\n",
    "        for k, v in data.items():\n",
    "            out[k.replace(\"\\\\\", \"/\")] = int(v)\n",
    "        return out\n",
    "\n",
    "    # -------------------------\n",
    "    # List format: [{\"image\": ..., \"no_contraband\": 0/1, \"isolated_items\": 0/1}]\n",
    "    # -------------------------\n",
    "    if isinstance(data, list):\n",
    "        out = {}\n",
    "        bad = []\n",
    "\n",
    "        for item in data:\n",
    "            filepath = item[\"image\"]\n",
    "            no_contra = int(item.get(\"no_contraband\", 0))\n",
    "            isolated = int(item.get(\"isolated_items\", 0))\n",
    "            empty = int(item.get(\"empty\", 0))\n",
    "\n",
    "            # Expected combos for your current binary task:\n",
    "            # (no_contra=1, isolated=0) -> empty tray -> class 0\n",
    "            # (no_contra=1, isolated=1) -> isolated benign items -> class 1\n",
    "            if no_contra == 1 and isolated == 0 and empty == 1:\n",
    "                class_id = 0\n",
    "            elif no_contra == 1 and isolated == 1 and empty == 0:\n",
    "                class_id = 1\n",
    "            else:\n",
    "                # Anything else is inconsistent for this binary stage setup\n",
    "                # e.g. no_contraband=0 implies contraband/mixed stage etc.\n",
    "                bad.append((filepath, no_contra, isolated))\n",
    "                continue\n",
    "\n",
    "            out[filepath] = class_id\n",
    "\n",
    "        if bad:\n",
    "            print(\"Found inconsistent labels (showing up to 10):\")\n",
    "            for row in bad[:10]:\n",
    "                print(\"  \", row)\n",
    "            raise ValueError(f\"Inconsistent labels for {len(bad)} samples in {label_path}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    raise ValueError(\"Label JSON must be a dict {filepath: class_id} or a list of items\")\n",
    "\n",
    "# Load\n",
    "train_label_map = load_label_map(TRAIN_LABELS_JSON)\n",
    "val_label_map   = load_label_map(VAL_LABELS_JSON)\n",
    "\n",
    "print(\"Train labels:\", len(train_label_map))\n",
    "print(\"Val labels:\", len(val_label_map))\n",
    "\n",
    "print(\"Train label distribution:\", Counter(train_label_map.values()))\n",
    "print(\"Val label distribution:\", Counter(val_label_map.values()))\n",
    "\n",
    "# Show a couple samples\n",
    "for i, (k, v) in enumerate(train_label_map.items()):\n",
    "    print(\"Example train label:\", k, \"->\", v)\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a89e6e",
   "metadata": {},
   "source": [
    "Define Dataset (respects split, no leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a562653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedSplitDataset(Dataset):\n",
    "    def __init__(self, index_rows, processed_root: Path, split: str, label_map: dict, transform=None):\n",
    "        self.processed_root = processed_root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "        # Filter filepaths by split\n",
    "        filepaths = []\n",
    "        for r in index_rows:\n",
    "            fp = r[\"filepath\"].replace(\"\\\\\", \"/\")\n",
    "            row_split = (r.get(\"split\") or \"\").strip().lower()\n",
    "\n",
    "            if row_split:\n",
    "                if row_split == split:\n",
    "                    filepaths.append(fp)\n",
    "            else:\n",
    "                # fallback if index.csv doesn't have split column\n",
    "                if f\"images/{split}/\" in fp:\n",
    "                    filepaths.append(fp)\n",
    "\n",
    "        if not filepaths:\n",
    "            raise ValueError(f\"No samples found for split='{split}'\")\n",
    "\n",
    "        # Keep only those that have labels (strict)\n",
    "        self.filepaths = []\n",
    "        self.labels = []\n",
    "        for fp in filepaths:\n",
    "            filename = Path(fp).name\n",
    "            label = self.label_map.get(filename, 0)  # default to 0 if missing\n",
    "            self.filepaths.append(fp)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.filepaths[idx]\n",
    "        img_path = self.processed_root / rel_path\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Force consistent mode\n",
    "        if IMAGE_MODE == \"gray\":\n",
    "            img = img.convert(\"L\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56b3e9",
   "metadata": {},
   "source": [
    "Transforms + DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a58308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1063\n",
      "Val samples: 120\n",
      "Batch image shape: torch.Size([70, 3, 256, 256]) | Batch label shape: torch.Size([70])\n"
     ]
    }
   ],
   "source": [
    "if IMAGE_MODE == \"gray\":\n",
    "    transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),  # -> [1, H, W]\n",
    "    ])\n",
    "    in_channels = 1\n",
    "else:\n",
    "    transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),  # -> [3, H, W]\n",
    "    ])\n",
    "    in_channels = 3\n",
    "\n",
    "train_ds = ProcessedSplitDataset(index_rows, PROCESSED_ROOT, \"train\", train_label_map, transform=transform)\n",
    "val_ds   = ProcessedSplitDataset(index_rows, PROCESSED_ROOT, \"val\",   val_label_map,   transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples:\", len(val_ds))\n",
    "\n",
    "# Quick check tensor shapes\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch image shape:\", x.shape, \"| Batch label shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec6184",
   "metadata": {},
   "source": [
    "Train with the previous model from previous stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbb0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN_GAP(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # /2\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),          # /4\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),          # /8\n",
    "        )\n",
    "        # IMAGE_SIZE is global; compute flattened size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, IMAGE_SIZE, IMAGE_SIZE)\n",
    "            out = self.features(dummy)\n",
    "            flat_dim = out.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# LESSER PARAMETER (BETTER FOR SMALL DATASET)\n",
    "class SimpleCNN_GAP(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "\n",
    "        # Global Average Pool: (B, 64, H, W) -> (B, 64, 1, 1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1) # (B, 64)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "\n",
    "#model = SimpleCNN(in_channels=in_channels, num_classes=NUM_CLASSES).to(device)\n",
    "model = SimpleCNN_GAP(in_channels=in_channels,num_classes=NUM_CLASSES).to(device)\n",
    "#model.load_state_dict(torch.load(\"../../models/classifier/Stage0/modelV2.pt\")) #IMPORTANT, LOAD THE MODEL FROM PREVIOUS STAGE\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0bbb8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27874"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42f854",
   "metadata": {},
   "source": [
    "Training Loop (train + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec5860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] Train Loss: 3.3005 | Train Acc: 0.696 | Val Loss: 0.6491 | Val Acc: 0.708\n",
      "Epoch [2/15] Train Loss: 0.6489 | Train Acc: 0.695 | Val Loss: 0.6595 | Val Acc: 0.708\n",
      "Epoch [3/15] Train Loss: 0.6666 | Train Acc: 0.678 | Val Loss: 0.6580 | Val Acc: 0.708\n",
      "Epoch [4/15] Train Loss: 0.6552 | Train Acc: 0.692 | Val Loss: 0.6374 | Val Acc: 0.708\n",
      "Epoch [5/15] Train Loss: 0.6383 | Train Acc: 0.695 | Val Loss: 0.6214 | Val Acc: 0.708\n",
      "Epoch [6/15] Train Loss: 0.6273 | Train Acc: 0.696 | Val Loss: 0.6148 | Val Acc: 0.708\n",
      "Epoch [7/15] Train Loss: 0.6290 | Train Acc: 0.696 | Val Loss: 0.6139 | Val Acc: 0.708\n",
      "Epoch [8/15] Train Loss: 0.6256 | Train Acc: 0.696 | Val Loss: 0.6135 | Val Acc: 0.708\n",
      "Epoch [9/15] Train Loss: 0.6321 | Train Acc: 0.696 | Val Loss: 0.6126 | Val Acc: 0.708\n",
      "Epoch [10/15] Train Loss: 0.6318 | Train Acc: 0.696 | Val Loss: 0.6126 | Val Acc: 0.708\n",
      "Epoch [11/15] Train Loss: 0.6297 | Train Acc: 0.696 | Val Loss: 0.6123 | Val Acc: 0.708\n",
      "Epoch [12/15] Train Loss: 0.6290 | Train Acc: 0.696 | Val Loss: 0.6117 | Val Acc: 0.708\n",
      "Epoch [13/15] Train Loss: 0.6286 | Train Acc: 0.696 | Val Loss: 0.6133 | Val Acc: 0.708\n",
      "Epoch [14/15] Train Loss: 0.6271 | Train Acc: 0.696 | Val Loss: 0.6109 | Val Acc: 0.708\n",
      "Epoch [15/15] Train Loss: 0.6306 | Train Acc: 0.696 | Val Loss: 0.6108 | Val Acc: 0.708\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss_sum, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss = train_loss_sum / max(1, train_total)\n",
    "    train_acc  = train_correct / max(1, train_total)\n",
    "\n",
    "    # ---- Val ----\n",
    "    model.eval()\n",
    "    val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss_sum += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / max(1, val_total)\n",
    "    val_acc  = val_correct / max(1, val_total)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch}/{EPOCHS}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f} \"\n",
    "        f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9d2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a09e6a",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1e7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: ../../models/classifier/Stage1/model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), MODEL_OUT_PATH)\n",
    "print(\"✅ Saved model to:\", MODEL_OUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray-gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
