{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d87f3d7",
   "metadata": {},
   "source": [
    "SPLIT THEM TO 3 FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797f6234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 875 images\n",
      "Groups total: 175\n",
      "Group split -> train/val/test: 148/17/10\n",
      "Image split -> train/val/test: 740/85/50\n",
      "âœ… Dataset frozen with GROUP split (aug/orig siblings stay together)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "INTERIM_DIR = Path(\"../../data/interim/Stage1/color_clahe_1500x1000_noborder_aug\")  # source (contains *_orig.png and *_augXX.png)\n",
    "PROCESSED_DIR = Path(\"../../data/processed/Stage1\")\n",
    "\n",
    "IMAGES_DIR = PROCESSED_DIR / \"images\"\n",
    "TRAIN_DIR = IMAGES_DIR / \"train\"\n",
    "VAL_DIR   = IMAGES_DIR / \"val\"\n",
    "TEST_DIR  = IMAGES_DIR / \"test\"\n",
    "\n",
    "TRAIN_RATIO = 0.85\n",
    "VAL_RATIO   = 0.10\n",
    "TEST_RATIO  = 0.05\n",
    "SEED = 42\n",
    "\n",
    "IMAGE_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\"}\n",
    "\n",
    "# =========================\n",
    "# SETUP\n",
    "# =========================\n",
    "random.seed(SEED)\n",
    "\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "image_paths = sorted([p for p in INTERIM_DIR.iterdir() if p.suffix.lower() in IMAGE_EXTS])\n",
    "assert len(image_paths) > 0, \"No images found in interim directory\"\n",
    "print(f\"Found {len(image_paths)} images\")\n",
    "\n",
    "# =========================\n",
    "# GROUPING (prevents leakage)\n",
    "# =========================\n",
    "# Group all siblings together:\n",
    "#   base_orig.png, base_aug00.png, base_aug01.png ... -> same group \"base\"\n",
    "def group_id_from_filename(filename: str) -> str:\n",
    "    stem = Path(filename).stem\n",
    "    # remove trailing _orig or _augXX\n",
    "    stem = re.sub(r\"_(orig|aug\\d{2})$\", \"\", stem)\n",
    "    return stem\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for p in image_paths:\n",
    "    gid = group_id_from_filename(p.name)\n",
    "    groups[gid].append(p)\n",
    "\n",
    "group_ids = list(groups.keys())\n",
    "random.shuffle(group_ids)\n",
    "\n",
    "n_groups = len(group_ids)\n",
    "n_train_g = int(n_groups * TRAIN_RATIO)\n",
    "n_val_g   = int(n_groups * VAL_RATIO)\n",
    "\n",
    "train_gids = set(group_ids[:n_train_g])\n",
    "val_gids   = set(group_ids[n_train_g:n_train_g + n_val_g])\n",
    "test_gids  = set(group_ids[n_train_g + n_val_g:])\n",
    "\n",
    "train_imgs = [p for gid in train_gids for p in groups[gid]]\n",
    "val_imgs   = [p for gid in val_gids   for p in groups[gid]]\n",
    "test_imgs  = [p for gid in test_gids  for p in groups[gid]]\n",
    "\n",
    "print(f\"Groups total: {n_groups}\")\n",
    "print(f\"Group split -> train/val/test: {len(train_gids)}/{len(val_gids)}/{len(test_gids)}\")\n",
    "print(f\"Image split -> train/val/test: {len(train_imgs)}/{len(val_imgs)}/{len(test_imgs)}\")\n",
    "\n",
    "# =========================\n",
    "# COPY IMAGES\n",
    "# =========================\n",
    "def copy_split(imgs, split_dir, split_name):\n",
    "    relpaths = []\n",
    "    for p in imgs:\n",
    "        dst = split_dir / p.name\n",
    "        shutil.copy2(p, dst)\n",
    "        relpaths.append(f\"images/{split_name}/{p.name}\")\n",
    "    return relpaths\n",
    "\n",
    "train_rel = copy_split(train_imgs, TRAIN_DIR, \"train\")\n",
    "val_rel   = copy_split(val_imgs,   VAL_DIR,   \"val\")\n",
    "test_rel  = copy_split(test_imgs,  TEST_DIR,  \"test\")\n",
    "\n",
    "# =========================\n",
    "# SAVE splits.json\n",
    "# =========================\n",
    "splits = {\"train\": train_rel, \"val\": val_rel, \"test\": test_rel}\n",
    "\n",
    "with open(PROCESSED_DIR / \"splits.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            \"source\": str(INTERIM_DIR),\n",
    "            \"seed\": SEED,\n",
    "            \"ratios\": {\"train\": TRAIN_RATIO, \"val\": VAL_RATIO, \"test\": TEST_RATIO},\n",
    "            \"counts\": {k: len(v) for k, v in splits.items()},\n",
    "            \"grouping\": \"base_name (strip _orig/_augXX)\",\n",
    "            \"n_groups\": n_groups,\n",
    "            \"splits\": splits,\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# INDEX.CSV\n",
    "# =========================\n",
    "def extract_timestamp(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Matches: YYYY-MM-DD_HH-MM-SS-sss.png\n",
    "    (Your augmented files may NOT have this; it's okay if blank.)\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})-(\\d{3})\", filename)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return f\"{m.group(1)}T{m.group(2).replace('-', ':')}.{m.group(3)}\"\n",
    "\n",
    "with open(PROCESSED_DIR / \"index.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"filepath\",\n",
    "            \"split\",\n",
    "            \"stage\",\n",
    "            \"scan_session_id\",\n",
    "            \"group_id\",\n",
    "            \"filename\",\n",
    "            \"scan_timestamp\",\n",
    "            \"source_interim_path\",\n",
    "        ],\n",
    "    )\n",
    "    writer.writeheader()\n",
    "\n",
    "    for split_name, imgs in [(\"train\", train_imgs), (\"val\", val_imgs), (\"test\", test_imgs)]:\n",
    "        for p in imgs:\n",
    "            gid = group_id_from_filename(p.name)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"filepath\": f\"images/{split_name}/{p.name}\",\n",
    "                    \"split\": split_name,\n",
    "                    \"stage\": \"Stage1\",\n",
    "                    \"scan_session_id\": INTERIM_DIR.name,\n",
    "                    \"group_id\": gid,\n",
    "                    \"filename\": p.name,\n",
    "                    \"scan_timestamp\": extract_timestamp(p.name),\n",
    "                    \"source_interim_path\": str(p),\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(\"âœ… Dataset frozen with GROUP split (aug/orig siblings stay together)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e308afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 323 Stage0 images into Stage1/train\n",
      "Copied 35 Stage0 images into Stage1/val\n",
      "Copied 17 Stage0 images into Stage1/test\n",
      " Updated Stage1/splits.json with Stage0 appended\n",
      "âœ… Appended Stage0 rows into Stage1/index.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ADD-ON: MERGE Stage0 processed splits into Stage1 processed splits\n",
    "# âœ… NO Stage0__ / Stage0_ naming\n",
    "# âœ… filenames stay as: 2026-...png\n",
    "# âœ… safe: avoids overwriting collisions\n",
    "# âœ… updates Stage1/splits.json + Stage1/index.csv\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# -------------------------\n",
    "# REQUIRED: these must already exist in your notebook/script\n",
    "# -------------------------\n",
    "# IMAGES_DIR   = Path(\"../../data/processed/Stage1/images\")\n",
    "# PROCESSED_DIR = Path(\"../../data/processed/Stage1\")\n",
    "# IMAGE_EXTS   = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\"}\n",
    "# group_id_from_filename(filename: str) -> str\n",
    "# extract_timestamp(filename: str) -> str\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "STAGE0_PROCESSED_DIR = Path(\"../../data/processed/Stage0\")\n",
    "STAGE0_IMAGES_DIR = STAGE0_PROCESSED_DIR / \"images\"\n",
    "\n",
    "STAGE1_PROCESSED_DIR = Path(\"../../data/processed/Stage1\")\n",
    "PROCESSED_DIR = STAGE1_PROCESSED_DIR\n",
    "IMAGES_DIR = STAGE1_PROCESSED_DIR / \"images\"\n",
    "\n",
    "IMAGE_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\"}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# COPY (NO RENAMING)\n",
    "# -------------------------\n",
    "def copy_stage0_into_stage1(split_name: str):\n",
    "    src_dir = STAGE0_IMAGES_DIR / split_name\n",
    "    dst_dir = IMAGES_DIR / split_name\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not src_dir.exists():\n",
    "        print(f\"Stage0 split folder not found: {src_dir}\")\n",
    "        return []\n",
    "\n",
    "    copied_relpaths = []\n",
    "    skipped_collision = 0\n",
    "\n",
    "    for p in sorted(src_dir.iterdir()):\n",
    "        if p.suffix.lower() not in IMAGE_EXTS:\n",
    "            continue\n",
    "\n",
    "        # âœ… keep exact filename (no Stage0 prefix)\n",
    "        out_name = p.name\n",
    "        dst = dst_dir / out_name\n",
    "\n",
    "        if dst.exists():\n",
    "            # collision or already copied â€” skip to avoid overwriting\n",
    "            skipped_collision += 1\n",
    "            continue\n",
    "\n",
    "        shutil.copy2(p, dst)\n",
    "        copied_relpaths.append(f\"images/{split_name}/{out_name}\")\n",
    "\n",
    "    print(f\"Copied {len(copied_relpaths)} Stage0 images into Stage1/{split_name}\")\n",
    "    if skipped_collision:\n",
    "        print(f\"Skipped {skipped_collision} because filename already existed in Stage1/{split_name}\")\n",
    "    return copied_relpaths\n",
    "\n",
    "\n",
    "# Copy Stage0 -> Stage1 for each split\n",
    "stage0_train_rel = copy_stage0_into_stage1(\"train\")\n",
    "stage0_val_rel   = copy_stage0_into_stage1(\"val\")\n",
    "stage0_test_rel  = copy_stage0_into_stage1(\"test\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OPTIONAL: Update Stage1 splits.json\n",
    "# -------------------------\n",
    "splits_path = PROCESSED_DIR / \"splits.json\"\n",
    "if splits_path.exists():\n",
    "    with open(splits_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    meta.setdefault(\"splits\", {}).setdefault(\"train\", [])\n",
    "    meta.setdefault(\"splits\", {}).setdefault(\"val\", [])\n",
    "    meta.setdefault(\"splits\", {}).setdefault(\"test\", [])\n",
    "\n",
    "    meta[\"splits\"][\"train\"].extend(stage0_train_rel)\n",
    "    meta[\"splits\"][\"val\"].extend(stage0_val_rel)\n",
    "    meta[\"splits\"][\"test\"].extend(stage0_test_rel)\n",
    "\n",
    "    meta[\"counts\"] = {k: len(v) for k, v in meta[\"splits\"].items()}\n",
    "    meta.setdefault(\"sources\", [])\n",
    "    if str(STAGE0_PROCESSED_DIR) not in meta[\"sources\"]:\n",
    "        meta[\"sources\"].append(str(STAGE0_PROCESSED_DIR))\n",
    "\n",
    "    with open(splits_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(\" Updated Stage1/splits.json with Stage0 appended\")\n",
    "else:\n",
    "    print(\" Stage1/splits.json not found; skipped updating it\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OPTIONAL: Append Stage0 rows into Stage1 index.csv\n",
    "# (NO Stage0 naming; mark stage as Stage01_combined)\n",
    "# -------------------------\n",
    "index_path = PROCESSED_DIR / \"index.csv\"\n",
    "if index_path.exists():\n",
    "    # Read existing rows so we don't duplicate\n",
    "    existing = set()\n",
    "    with open(index_path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            existing.add(row[\"filepath\"])\n",
    "\n",
    "    def append_index_rows(split_name: str, relpaths: list):\n",
    "        with open(index_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    \"filepath\",\n",
    "                    \"split\",\n",
    "                    \"stage\",\n",
    "                    \"scan_session_id\",\n",
    "                    \"group_id\",\n",
    "                    \"filename\",\n",
    "                    \"scan_timestamp\",\n",
    "                    \"source_interim_path\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            for rel in relpaths:\n",
    "                if rel in existing:\n",
    "                    continue\n",
    "\n",
    "                filename = Path(rel).name  # âœ… still 2026-...png\n",
    "\n",
    "                writer.writerow(\n",
    "                    {\n",
    "                        \"filepath\": rel,\n",
    "                        \"split\": split_name,\n",
    "                        # don't label as Stage0 (prevents â€œStage0 namingâ€ vibe downstream)\n",
    "                        \"stage\": \"Stage01_combined\",\n",
    "                        \"scan_session_id\": \"merged_stage0_stage1\",\n",
    "                        \"group_id\": group_id_from_filename(filename),\n",
    "                        \"filename\": filename,\n",
    "                        \"scan_timestamp\": extract_timestamp(filename),\n",
    "                        # point back to Stage0 source file (no weird replace)\n",
    "                        \"source_interim_path\": str(STAGE0_IMAGES_DIR / split_name / filename),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    append_index_rows(\"train\", stage0_train_rel)\n",
    "    append_index_rows(\"val\", stage0_val_rel)\n",
    "    append_index_rows(\"test\", stage0_test_rel)\n",
    "\n",
    "    print(\"âœ… Appended Stage0 rows into Stage1/index.csv\")\n",
    "else:\n",
    "    print(\"âš ï¸ Stage1/index.csv not found; skipped appending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977bfef5",
   "metadata": {},
   "source": [
    "Automate Labelling for Stage1 since all data will have same labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a7e72",
   "metadata": {},
   "source": [
    "For Train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcc12a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JSON generated with 1063 samples â†’ ../../data/labels/Stage1/train.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "IMG_DIR = Path(\"../../data/processed/Stage1/images/train\")\n",
    "OUT_JSON = Path(\"../../data/labels/Stage1/train.json\")\n",
    "\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "annotations = []\n",
    "\n",
    "for p in sorted(IMG_DIR.iterdir()):\n",
    "    if p.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "        annotations.append({\n",
    "            \"image\": p.name,\n",
    "            \"no_contraband\": 1,\n",
    "            \"isolated_items\": 1,\n",
    "            \"empty\": 0\n",
    "        })\n",
    "\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(annotations, f, indent=2)\n",
    "\n",
    "print(f\"âœ… JSON generated with {len(annotations)} samples â†’ {OUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9819568",
   "metadata": {},
   "source": [
    "For Val.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d01b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JSON generated with 120 samples â†’ ../../data/labels/Stage1/val.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "IMG_DIR = Path(\"../../data/processed/Stage1/images/val\")\n",
    "OUT_JSON = Path(\"../../data/labels/Stage1/val.json\")\n",
    "\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "annotations = []\n",
    "\n",
    "for p in sorted(IMG_DIR.iterdir()):\n",
    "    if p.suffix.lower() in {\".png\", \".jpg\", \".jpeg\"}:\n",
    "        annotations.append({\n",
    "            \"image\": p.name,\n",
    "            \"no_contraband\": 1,\n",
    "            \"isolated_items\": 1,\n",
    "            \"empty\": 0\n",
    "        })\n",
    "\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(annotations, f, indent=2)\n",
    "\n",
    "print(f\"âœ… JSON generated with {len(annotations)} samples â†’ {OUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c664119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Split: train\n",
      "Stage0: 323 | /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage0/train.json\n",
      "Stage1: 1063 | /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/train.json\n",
      "Merged: 1386 (should be Stage1 + Stage0)\n",
      "ðŸ“ Wrote: /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/combined/train.json | items=1386\n",
      "\n",
      "==============================\n",
      "Split: val\n",
      "Stage0: 35 | /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage0/val.json\n",
      "Stage1: 120 | /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/val.json\n",
      "Merged: 155 (should be Stage1 + Stage0)\n",
      "ðŸ“ Wrote: /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/combined/val.json | items=155\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "STAGE0_LABEL_DIR = Path(\"../../data/labels/Stage0\")\n",
    "STAGE1_LABEL_DIR = Path(\"../../data/labels/Stage1\")\n",
    "\n",
    "OUT_DIR = Path(\"../../data/labels/Stage1/combined\")\n",
    "SPLITS = [\"train\", \"val\"]  # add \"test\" if you want\n",
    "\n",
    "\n",
    "def load_json(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path.resolve()}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json(path: Path, data):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"ðŸ“ Wrote: {path.resolve()} | items={len(data)}\")\n",
    "\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Split:\", split)\n",
    "\n",
    "    s0_path = STAGE0_LABEL_DIR / f\"{split}.json\"\n",
    "    s1_path = STAGE1_LABEL_DIR / f\"{split}.json\"\n",
    "    out_path = OUT_DIR / f\"{split}.json\"\n",
    "\n",
    "    s0 = load_json(s0_path)\n",
    "    s1 = load_json(s1_path)\n",
    "\n",
    "    print(\"Stage0:\", len(s0), \"|\", s0_path.resolve())\n",
    "    print(\"Stage1:\", len(s1), \"|\", s1_path.resolve())\n",
    "\n",
    "    # âœ… just merge lists, keep everything exactly as-is\n",
    "    merged = list(s1) + list(s0)\n",
    "\n",
    "    # optional: quick sanity counts\n",
    "    print(\"Merged:\", len(merged), \"(should be Stage1 + Stage0)\")\n",
    "\n",
    "    save_json(out_path, merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87d8800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/train.json\n",
      "Count: 1063\n",
      "First 3: [{'image': '2026-01-21_10-32-05-130_aug00.png', 'no_contraband': 1, 'isolated_items': 1, 'empty': 0}, {'image': '2026-01-21_10-32-05-130_aug000.png', 'no_contraband': 1, 'isolated_items': 1, 'empty': 0}, {'image': '2026-01-21_10-32-05-130_aug001.png', 'no_contraband': 1, 'isolated_items': 1, 'empty': 0}]\n",
      "Any Stage0__ entries? False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "p = Path(\"../../data/labels/Stage1/train.json\")\n",
    "print(\"Reading:\", p.resolve())\n",
    "with open(p, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Count:\", len(data))\n",
    "print(\"First 3:\", data[:3])\n",
    "print(\"Any Stage0__ entries?\", any(x[\"image\"].startswith(\"Stage0__\") for x in data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e55b8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Stage1 processed split folders: ../../data/processed/Stage1/images/train ../../data/processed/Stage1/images/val ../../data/processed/Stage1/images/test\n",
      "Stage1 interim images found: 875\n",
      "Stage1 groups total: 175\n",
      "Stage1 group split -> train/val/test: 148/17/10\n",
      "Stage1 image split -> train/val/test: 740/85/50\n",
      "Copied Stage1 images into processed splits.\n",
      "Copied Stage0 images into Stage1 processed splits (no renaming).\n",
      "Wrote: /home/ssy/Desktop/xray-gen-ai_Project/data/processed/Stage1/splits.json\n",
      "Wrote: /home/ssy/Desktop/xray-gen-ai_Project/data/processed/Stage1/index.csv | rows: 2056\n",
      "Auto labels: /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/train.json | items=1715\n",
      "Auto labels: /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/val.json | items=221\n",
      "Merged labels -> /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/combined/train.json | Stage1=1715 + Stage0=975 = 2690\n",
      "Merged labels -> /home/ssy/Desktop/xray-gen-ai_Project/data/labels/Stage1/combined/val.json | Stage1=221 + Stage0=136 = 357\n",
      "\n",
      "DONE: Stage1 processed now contains Stage1+Stage0 images in 3 folders, and splits/index/labels are updated.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "# Stage1 interim source (raw augmented outputs)\n",
    "STAGE1_INTERIM_DIR = Path(\"../../data/interim/Stage1/color_clahe_1500x1000_noborder_aug\")\n",
    "\n",
    "# Stage1 processed output (the combined dataset will live here)\n",
    "STAGE1_PROCESSED_DIR = Path(\"../../data/processed/Stage1\")\n",
    "STAGE1_IMAGES_DIR = STAGE1_PROCESSED_DIR / \"images\"\n",
    "TRAIN_DIR = STAGE1_IMAGES_DIR / \"train\"\n",
    "VAL_DIR   = STAGE1_IMAGES_DIR / \"val\"\n",
    "TEST_DIR  = STAGE1_IMAGES_DIR / \"test\"\n",
    "\n",
    "# Stage0 processed source to merge in (already split)\n",
    "STAGE0_PROCESSED_DIR = Path(\"../../data/processed/Stage0\")\n",
    "STAGE0_IMAGES_DIR = STAGE0_PROCESSED_DIR / \"images\"\n",
    "\n",
    "# Labels dirs\n",
    "STAGE0_LABEL_DIR = Path(\"../../data/labels/Stage0\")\n",
    "STAGE1_LABEL_DIR = Path(\"../../data/labels/Stage1\")\n",
    "COMBINED_LABEL_DIR = Path(\"../../data/labels/Stage1/combined\")\n",
    "\n",
    "# Split ratios for Stage1 (group-safe)\n",
    "TRAIN_RATIO = 0.85\n",
    "VAL_RATIO   = 0.10\n",
    "TEST_RATIO  = 0.05\n",
    "SEED = 42\n",
    "\n",
    "IMAGE_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\"}\n",
    "\n",
    "# Stage1 auto-labels (your Stage1 is isolated benign items)\n",
    "STAGE1_AUTO_LABEL = {\n",
    "    \"no_contraband\": 1,\n",
    "    \"isolated_items\": 1,\n",
    "    \"empty\": 0,\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_empty_dir(d: Path):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    # clear folder\n",
    "    for p in d.iterdir():\n",
    "        if p.is_file():\n",
    "            p.unlink()\n",
    "        elif p.is_dir():\n",
    "            shutil.rmtree(p)\n",
    "\n",
    "def group_id_from_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Groups siblings together:\n",
    "      base_orig.png, base_aug00.png, base_aug01.png -> same group \"base\"\n",
    "    \"\"\"\n",
    "    stem = Path(filename).stem\n",
    "    stem = re.sub(r\"_(orig|aug\\d{2})$\", \"\", stem)\n",
    "    return stem\n",
    "\n",
    "def extract_timestamp(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Matches: YYYY-MM-DD_HH-MM-SS-sss\n",
    "    returns ISO-like string, else \"\".\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})-(\\d{3})\", filename)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return f\"{m.group(1)}T{m.group(2).replace('-', ':')}.{m.group(3)}\"\n",
    "\n",
    "def load_json_list(path: Path):\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(f\"Expected list JSON at {path}, got {type(data)}\")\n",
    "    return data\n",
    "\n",
    "def save_json_list(path: Path, data_list):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data_list, f, indent=2)\n",
    "\n",
    "def copy_images_into_split(src_paths, dst_dir: Path, split_name: str):\n",
    "    \"\"\"\n",
    "    Copy images into Stage1 processed split folder WITHOUT renaming.\n",
    "    Skips if filename already exists in destination.\n",
    "    Returns list of relpaths (images/{split}/{filename})\n",
    "    \"\"\"\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    relpaths = []\n",
    "    skipped = 0\n",
    "    for p in sorted(src_paths, key=lambda x: x.name):\n",
    "        if p.suffix.lower() not in IMAGE_EXTS:\n",
    "            continue\n",
    "        dst = dst_dir / p.name\n",
    "        if dst.exists():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        shutil.copy2(p, dst)\n",
    "        relpaths.append(f\"images/{split_name}/{p.name}\")\n",
    "    return relpaths, skipped\n",
    "\n",
    "def write_index_csv(index_path: Path, rows: list):\n",
    "    \"\"\"\n",
    "    Overwrite index.csv with provided rows.\n",
    "    \"\"\"\n",
    "    index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(index_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\n",
    "                \"filepath\",\n",
    "                \"split\",\n",
    "                \"stage\",\n",
    "                \"scan_session_id\",\n",
    "                \"group_id\",\n",
    "                \"filename\",\n",
    "                \"scan_timestamp\",\n",
    "                \"source_interim_path\",\n",
    "            ],\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        for r in rows:\n",
    "            writer.writerow(r)\n",
    "\n",
    "# =========================\n",
    "# 0) SETUP OUTPUT FOLDERS (clear Stage1 processed)\n",
    "# =========================\n",
    "random.seed(SEED)\n",
    "\n",
    "ensure_empty_dir(TRAIN_DIR)\n",
    "ensure_empty_dir(VAL_DIR)\n",
    "ensure_empty_dir(TEST_DIR)\n",
    "\n",
    "print(\"Cleared Stage1 processed split folders:\", TRAIN_DIR, VAL_DIR, TEST_DIR)\n",
    "\n",
    "# =========================\n",
    "# 1) SPLIT Stage1 INTERIM INTO 3 FOLDERS (group-safe)\n",
    "# =========================\n",
    "stage1_image_paths = sorted([p for p in STAGE1_INTERIM_DIR.iterdir() if p.suffix.lower() in IMAGE_EXTS])\n",
    "if not stage1_image_paths:\n",
    "    raise ValueError(f\"No images found in {STAGE1_INTERIM_DIR.resolve()}\")\n",
    "print(f\"Stage1 interim images found: {len(stage1_image_paths)}\")\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for p in stage1_image_paths:\n",
    "    gid = group_id_from_filename(p.name)\n",
    "    groups[gid].append(p)\n",
    "\n",
    "group_ids = list(groups.keys())\n",
    "random.shuffle(group_ids)\n",
    "\n",
    "n_groups = len(group_ids)\n",
    "n_train_g = int(n_groups * TRAIN_RATIO)\n",
    "n_val_g   = int(n_groups * VAL_RATIO)\n",
    "\n",
    "train_gids = set(group_ids[:n_train_g])\n",
    "val_gids   = set(group_ids[n_train_g:n_train_g + n_val_g])\n",
    "test_gids  = set(group_ids[n_train_g + n_val_g:])\n",
    "\n",
    "stage1_train_imgs = [p for gid in train_gids for p in groups[gid]]\n",
    "stage1_val_imgs   = [p for gid in val_gids   for p in groups[gid]]\n",
    "stage1_test_imgs  = [p for gid in test_gids  for p in groups[gid]]\n",
    "\n",
    "print(f\"Stage1 groups total: {n_groups}\")\n",
    "print(f\"Stage1 group split -> train/val/test: {len(train_gids)}/{len(val_gids)}/{len(test_gids)}\")\n",
    "print(f\"Stage1 image split -> train/val/test: {len(stage1_train_imgs)}/{len(stage1_val_imgs)}/{len(stage1_test_imgs)}\")\n",
    "\n",
    "stage1_train_rel, s1_skip_tr = copy_images_into_split(stage1_train_imgs, TRAIN_DIR, \"train\")\n",
    "stage1_val_rel,   s1_skip_va = copy_images_into_split(stage1_val_imgs,   VAL_DIR,   \"val\")\n",
    "stage1_test_rel,  s1_skip_te = copy_images_into_split(stage1_test_imgs,  TEST_DIR,  \"test\")\n",
    "\n",
    "print(\"Copied Stage1 images into processed splits.\")\n",
    "if (s1_skip_tr + s1_skip_va + s1_skip_te) > 0:\n",
    "    print(\"Skipped Stage1 collisions (unexpected since folder was cleared):\",\n",
    "          s1_skip_tr, s1_skip_va, s1_skip_te)\n",
    "\n",
    "# =========================\n",
    "# 2) COPY Stage0 PROCESSED INTO SAME 3 FOLDERS (NO RENAMING)\n",
    "# =========================\n",
    "def stage0_split_paths(split_name: str):\n",
    "    d = STAGE0_IMAGES_DIR / split_name\n",
    "    if not d.exists():\n",
    "        print(f\"Stage0 split folder missing: {d.resolve()}\")\n",
    "        return []\n",
    "    return sorted([p for p in d.iterdir() if p.suffix.lower() in IMAGE_EXTS])\n",
    "\n",
    "s0_train_paths = stage0_split_paths(\"train\")\n",
    "s0_val_paths   = stage0_split_paths(\"val\")\n",
    "s0_test_paths  = stage0_split_paths(\"test\")\n",
    "\n",
    "stage0_train_rel, s0_skip_tr = copy_images_into_split(s0_train_paths, TRAIN_DIR, \"train\")\n",
    "stage0_val_rel,   s0_skip_va = copy_images_into_split(s0_val_paths,   VAL_DIR,   \"val\")\n",
    "stage0_test_rel,  s0_skip_te = copy_images_into_split(s0_test_paths,  TEST_DIR,  \"test\")\n",
    "\n",
    "print(\"Copied Stage0 images into Stage1 processed splits (no renaming).\")\n",
    "if (s0_skip_tr + s0_skip_va + s0_skip_te) > 0:\n",
    "    print(\"Skipped Stage0 collisions (means same filename already existed in Stage1):\",\n",
    "          s0_skip_tr, s0_skip_va, s0_skip_te)\n",
    "\n",
    "# =========================\n",
    "# 3) WRITE Stage1 splits.json (combined)\n",
    "# =========================\n",
    "combined_splits = {\n",
    "    \"train\": stage1_train_rel + stage0_train_rel,\n",
    "    \"val\":   stage1_val_rel   + stage0_val_rel,\n",
    "    \"test\":  stage1_test_rel  + stage0_test_rel,\n",
    "}\n",
    "\n",
    "splits_json = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"seed\": SEED,\n",
    "    \"ratios_stage1\": {\"train\": TRAIN_RATIO, \"val\": VAL_RATIO, \"test\": TEST_RATIO},\n",
    "    \"counts\": {k: len(v) for k, v in combined_splits.items()},\n",
    "    \"grouping_stage1\": \"base_name (strip _orig/_augXX)\",\n",
    "    \"n_groups_stage1\": n_groups,\n",
    "    \"sources\": [str(STAGE1_INTERIM_DIR), str(STAGE0_PROCESSED_DIR)],\n",
    "    \"splits\": combined_splits,\n",
    "}\n",
    "\n",
    "with open(STAGE1_PROCESSED_DIR / \"splits.json\", \"w\") as f:\n",
    "    json.dump(splits_json, f, indent=2)\n",
    "\n",
    "print(\"Wrote:\", (STAGE1_PROCESSED_DIR / \"splits.json\").resolve())\n",
    "\n",
    "# =========================\n",
    "# 4) WRITE Stage1 index.csv (combined)\n",
    "#    - includes Stage1 + Stage0 copied items\n",
    "# =========================\n",
    "index_rows = []\n",
    "\n",
    "def add_index_rows(relpaths, split_name, stage_name, scan_session_id, source_root: Path):\n",
    "    for rel in relpaths:\n",
    "        filename = Path(rel).name\n",
    "        index_rows.append(\n",
    "            {\n",
    "                \"filepath\": rel,\n",
    "                \"split\": split_name,\n",
    "                \"stage\": stage_name,\n",
    "                \"scan_session_id\": scan_session_id,\n",
    "                \"group_id\": group_id_from_filename(filename),\n",
    "                \"filename\": filename,\n",
    "                \"scan_timestamp\": extract_timestamp(filename),\n",
    "                \"source_interim_path\": str(source_root / split_name / filename) if (source_root / split_name).exists() else \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Stage1 rows (source is the interim dir itself, but we only have filename; keep absolute original if possible)\n",
    "# We'll set source_interim_path as STAGE1_INTERIM_DIR/<filename> (best effort).\n",
    "def add_stage1_index(relpaths, split_name):\n",
    "    for rel in relpaths:\n",
    "        filename = Path(rel).name\n",
    "        index_rows.append(\n",
    "            {\n",
    "                \"filepath\": rel,\n",
    "                \"split\": split_name,\n",
    "                \"stage\": \"Stage1\",\n",
    "                \"scan_session_id\": STAGE1_INTERIM_DIR.name,\n",
    "                \"group_id\": group_id_from_filename(filename),\n",
    "                \"filename\": filename,\n",
    "                \"scan_timestamp\": extract_timestamp(filename),\n",
    "                \"source_interim_path\": str(STAGE1_INTERIM_DIR / filename),\n",
    "            }\n",
    "        )\n",
    "\n",
    "add_stage1_index(stage1_train_rel, \"train\")\n",
    "add_stage1_index(stage1_val_rel, \"val\")\n",
    "add_stage1_index(stage1_test_rel, \"test\")\n",
    "\n",
    "# Stage0 rows\n",
    "add_index_rows(stage0_train_rel, \"train\", \"Stage0\", STAGE0_PROCESSED_DIR.name, STAGE0_IMAGES_DIR)\n",
    "add_index_rows(stage0_val_rel,   \"val\",   \"Stage0\", STAGE0_PROCESSED_DIR.name, STAGE0_IMAGES_DIR)\n",
    "add_index_rows(stage0_test_rel,  \"test\",  \"Stage0\", STAGE0_PROCESSED_DIR.name, STAGE0_IMAGES_DIR)\n",
    "\n",
    "index_path = STAGE1_PROCESSED_DIR / \"index.csv\"\n",
    "write_index_csv(index_path, index_rows)\n",
    "print(\"Wrote:\", index_path.resolve(), \"| rows:\", len(index_rows))\n",
    "\n",
    "# =========================\n",
    "# 5) AUTO-GENERATE Stage1 labels (train/val/test) based on Stage1 PROCESSED folders\n",
    "#    (All same labels you specified)\n",
    "# =========================\n",
    "def write_auto_label_json(img_dir: Path, out_json: Path):\n",
    "    anns = []\n",
    "    for p in sorted(img_dir.iterdir()):\n",
    "        if p.suffix.lower() in IMAGE_EXTS:\n",
    "            anns.append({\"image\": p.name, **STAGE1_AUTO_LABEL})\n",
    "    save_json_list(out_json, anns)\n",
    "    print(f\"Auto labels: {out_json.resolve()} | items={len(anns)}\")\n",
    "    return anns\n",
    "\n",
    "stage1_train_json = STAGE1_LABEL_DIR / \"train.json\"\n",
    "stage1_val_json   = STAGE1_LABEL_DIR / \"val.json\"\n",
    "#stage1_test_json  = STAGE1_LABEL_DIR / \"test.json\"\n",
    "\n",
    "_ = write_auto_label_json(TRAIN_DIR, stage1_train_json)\n",
    "_ = write_auto_label_json(VAL_DIR,   stage1_val_json)\n",
    "#_ = write_auto_label_json(TEST_DIR,  stage1_test_json)\n",
    "\n",
    "# =========================\n",
    "# 6) MERGE Stage1 + Stage0 label JSONs into combined/\n",
    "#    (keeps entries exactly as-is; no renaming; duplicates allowed)\n",
    "# =========================\n",
    "COMBINED_LABEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def merge_label_lists(stage1_path: Path, stage0_path: Path, out_path: Path):\n",
    "    s1 = load_json_list(stage1_path)\n",
    "    s0 = load_json_list(stage0_path)\n",
    "    merged = list(s1) + list(s0)  # EXACTLY as-is\n",
    "    save_json_list(out_path, merged)\n",
    "    print(f\"Merged labels -> {out_path.resolve()} | Stage1={len(s1)} + Stage0={len(s0)} = {len(merged)}\")\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    merge_label_lists(\n",
    "        STAGE1_LABEL_DIR / f\"{split}.json\",\n",
    "        STAGE0_LABEL_DIR / f\"{split}.json\",\n",
    "        COMBINED_LABEL_DIR / f\"{split}.json\",\n",
    "    )\n",
    "\n",
    "print(\"\\nDONE: Stage1 processed now contains Stage1+Stage0 images in 3 folders, and splits/index/labels are updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray-gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
